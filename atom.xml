<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PengWei</title>
  
  
  <link href="/note/atom.xml" rel="self"/>
  
  <link href="http://pengwei785.github.io/"/>
  <updated>2020-04-22T05:20:01.924Z</updated>
  <id>http://pengwei785.github.io/</id>
  
  <author>
    <name>Peng Wei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PCA降维</title>
    <link href="http://pengwei785.github.io/2020/04/22/PCA%E9%99%8D%E7%BB%B4/"/>
    <id>http://pengwei785.github.io/2020/04/22/PCA%E9%99%8D%E7%BB%B4/</id>
    <published>2020-04-22T05:20:01.000Z</published>
    <updated>2020-04-22T05:20:01.924Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>深度学习</title>
    <link href="http://pengwei785.github.io/2020/04/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://pengwei785.github.io/2020/04/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-04-22T05:19:45.000Z</published>
    <updated>2020-04-22T05:21:03.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><h2 id="什么是梯度？"><a href="#什么是梯度？" class="headerlink" title="什么是梯度？"></a>什么是梯度？</h2><p>导数：标量。各个反向都有，反映函数在给定方向的变化率。<br>偏导数：标量。是一种特殊的导数。沿自变量反向，反映函数在各自变量方向的变化率。<br>梯度：向量。由各个偏导数组成的向量。梯度的反向反映函数增长的方向</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419211738630-1707281905.png" alt=""></p><p>下面的箭头表示梯度：</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419212250493-1423645731.png" alt=""></p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419213056996-1361623212.gif" alt=""></p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419212953381-2018117993.gif" alt=""></p><p>图源：<a href="https://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">https://ruder.io/optimizing-gradient-descent/</a></p><h2 id="局部极小值"><a href="#局部极小值" class="headerlink" title="局部极小值"></a>局部极小值</h2><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419213459163-1025658897.png" alt=""></p><p>ResNet可以平滑局部极小值：</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419213541828-885651836.png" alt=""></p><h2 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h2><p>有可能求得的是某个维度的极小值，却是另一个维度的极大值。</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200420203208366-1835740731.png" alt=""></p><h1 id="影响求极小值的因素"><a href="#影响求极小值的因素" class="headerlink" title="影响求极小值的因素"></a>影响求极小值的因素</h1><h2 id="初始状态"><a href="#初始状态" class="headerlink" title="初始状态"></a>初始状态</h2><p>不同的初始状态对结果产生影响：</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419214308100-1539977211.png" alt=""></p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419214403385-614490166.png" alt=""></p><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>不再赘述</p><h2 id="动量"><a href="#动量" class="headerlink" title="动量"></a>动量</h2><p>使其具有保持原来运动方向的趋势。</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419214512089-650172177.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;梯度&quot;&gt;&lt;a href=&quot;#梯度&quot; class=&quot;headerlink&quot; title=&quot;梯度&quot;&gt;&lt;/a&gt;梯度&lt;/h1&gt;&lt;h2 id=&quot;什么是梯度？&quot;&gt;&lt;a href=&quot;#什么是梯度？&quot; class=&quot;headerlink&quot; title=&quot;什么是梯度？&quot;&gt;&lt;/a&gt;什
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://pengwei785.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="http://pengwei785.github.io/2020/04/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>http://pengwei785.github.io/2020/04/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-04-22T05:19:31.000Z</published>
    <updated>2020-04-22T05:20:26.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装Gym踩的坑"><a href="#安装Gym踩的坑" class="headerlink" title="安装Gym踩的坑"></a>安装Gym踩的坑</h1><p><a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym官网</a><br><a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">Gym官方文档</a></p><p>gym可以pip安装和源码安装，pip安装的是一些常用的游戏环境，并不是全部的。这里选择源码安装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/openai/gym</span><br><span class="line">cd gym</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p>执行<code>pip install -e .[all]</code>可安装完整的gym库。</p><p>windows下安装的时候可能会出现的问题：</p><ul><li>error: command ‘swig.exe’ failed: no such file or directory<br><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200418003605325-1612098545.png" alt=""></li></ul><p>解决办法1：conda安装swig <code>conda install swig</code><br>解决办法2：下载<a href="https://netix.dl.sourceforge.net/project/swig/swigwin/swigwin-3.0.12/swigwin-3.0.12.zip" target="_blank" rel="noopener">swig for windows</a></p><ol><li>把swig.exe复制到python安装文件(如C:/python37，如果是anaconda环境则是.conda/envs/环境名/)</li><li>把<code>swigwin-3.0.12/Lib</code>文件夹下所有 *.swg复制到<code>C:/python37/Lib或.conda/envs/环境名/Lib/</code></li><li>打开<code>swigwin-3.0.12/Lib/python</code>并复制所有文件到<code>C:/python27/Lib或.conda/envs/环境名/Lib/</code></li><li>复制’typemaps’文件夹到<code>C:/python27/Lib/</code><br>出现这个问题就是没有复制typemaps：<br><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200418003633434-1167350096.png" alt=""></li></ol><p>(并且要确保你安装了Microsoft Visual C++ Compiler for Python)<a href="http://go.microsoft.com/fwlink/?LinkId=691126" target="_blank" rel="noopener">Visual C++ Build Tools 2015</a></p><p>如果没有安装会报错：<br><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200418003325055-1523727167.png" alt=""><br>安装完成：<br><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200418003211864-1108347896.png" alt=""></p><p>然后安装cython：<code>conda install cython</code><br><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200418004002253-517243945.png" alt=""></p><p>最后Mujoco是收费的，如果是学生可以免费使用。mujoco是一个机器人仿真库，需要的同学可以安装：<br><a href="https://github.com/openai/mujoco-py#install-mujoco" target="_blank" rel="noopener">https://github.com/openai/mujoco-py#install-mujoco</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;安装Gym踩的坑&quot;&gt;&lt;a href=&quot;#安装Gym踩的坑&quot; class=&quot;headerlink&quot; title=&quot;安装Gym踩的坑&quot;&gt;&lt;/a&gt;安装Gym踩的坑&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://gym.openai.com/&quot; target=&quot;_bla
      
    
    </summary>
    
    
      <category term="强化学习" scheme="http://pengwei785.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>配置博客</title>
    <link href="http://pengwei785.github.io/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/"/>
    <id>http://pengwei785.github.io/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-04-21T12:40:56.000Z</published>
    <updated>2020-04-21T14:36:09.373Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<code>/themes/next/source/css/_common/scaffolding/base.styl  &gt;  a</code></p><p>字体：<code>/themes/next/source/css/_variables/base.styl  &gt;  font-size</code></p><p><img src="/note/note/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/Snipaste_2020-04-19_13-46-47.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;链接：&lt;code&gt;/themes/next/source/css/_common/scaffolding/base.styl  &amp;gt;  a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;字体：&lt;code&gt;/themes/next/source/css/_variables/base.st
      
    
    </summary>
    
    
      <category term="其它" scheme="http://pengwei785.github.io/categories/%E5%85%B6%E5%AE%83/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch</title>
    <link href="http://pengwei785.github.io/2020/04/21/PyTorch/"/>
    <id>http://pengwei785.github.io/2020/04/21/PyTorch/</id>
    <published>2020-04-21T06:53:16.000Z</published>
    <updated>2020-04-22T02:50:51.922Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch官网</a><br><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch官方教程</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch官方文档</a><br><a href="https://pytorch.apachecn.org/docs/1.4/" target="_blank" rel="noopener">PyTorch中文文档/教程</a><br><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">动手学深度学习PyTorch版</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。</p><a id="more"></a><p>寻找下面函数的最小值：</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419152635168-692192025.png" alt></p><p>conda：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">himmelblau</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>] - <span class="number">11</span>)**<span class="number">2</span> + (x[<span class="number">0</span>] + x[<span class="number">1</span>]**<span class="number">2</span> - <span class="number">7</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> plotly.graph_objects <span class="keyword">as</span> go</span><br><span class="line">x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">y = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># print('x,y range:', x.shape, y.shape)</span></span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">fig = go.Figure(data=go.Surface(z=himmelblau([X,Y])))</span><br><span class="line">fig.write_image(<span class="string">'figure2.svg'</span>)</span><br><span class="line">fig.write_html(<span class="string">'first_figure.html'</span>, auto_open=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = torch.tensor([<span class="number">0.</span>, <span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer = torch.optim.Adam([x], lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    pred = himmelblau(x)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度信息清零</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step() <span class="comment"># 每调用一次step，就更新一次: x' = x, y' = y</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.detach().numpy(), pred.item()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = tf.Variable([<span class="number">0.</span>, <span class="number">0.</span>])  <span class="comment"># 传入GradientTape计算梯度的必须是tf.Variable类型</span></span><br><span class="line">optimizer = tf.optimizers.Adam(lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch([x])</span><br><span class="line">        pred = himmelblau(x)</span><br><span class="line">        </span><br><span class="line">    grads = tape.gradient(pred, [x])</span><br><span class="line">    optimizer.apply_gradients(zip(grads, [x])) <span class="comment"># 和pytorch不同，tf是将所有梯度信息存起来一次性更新</span></span><br><span class="line">    <span class="comment"># x -= 0.001*grads</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.numpy(), pred.numpy()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br></pre></td></tr></table></figure><div class="tabs" id="unique-name"><ul class="nav-tabs"><li class="tab active"><a href="#unique-name-1">conda</a></li><li class="tab"><a href="#unique-name-2">pip</a></li></ul><div class="tab-content"><div class="tab-pane active" id="unique-name-1"><blockquote><p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.470422983169556</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 43.112674951553345</strong></p></blockquote></div><div class="tab-pane" id="unique-name-2"><blockquote><p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.337981462478638</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 54.814427614212036</strong></p></blockquote></div></div></div><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>新建环境：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name torch python=<span class="number">3.7</span></span><br></pre></td></tr></table></figure><p>安装一些可能要用到的包（非必须，看自己情况）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install numpy</span><br><span class="line">conda install spyder</span><br><span class="line">conda install jupyter notebook</span><br></pre></td></tr></table></figure><p>安装PyTorch：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cpuonly -c pytorch <span class="comment"># CPU版</span></span><br></pre></td></tr></table></figure><p>GPU版根据CUDA版本不同命令也不同，可以去<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">这里</a>查看安装命令</p><h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><p><strong>requires_grad</strong></p><p>设置张量的属性<code>.requires_grad</code> 为 <code>True</code>，那么它将会追踪对于该张量的所有操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>],<span class="number">2</span>)  <span class="comment"># 应为w = torch.full([1], 2, requires_grad=True)</span></span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure><blockquote><p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p></blockquote><div class="note warning no-icon">            <p>PyTorch的图是静态创建的，如果使用<code>requires_grad_()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure>依然会报错：> RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn<p>因为mse这张图已经创建好了，因此要重新创建一次，或者将mse的创建放到<code>w.requires_grad_(True)</code>后面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">grad = torch.autograd.grad(mse, [w])  <span class="comment"># 返回一个列表，分别是对每个变量的梯度</span></span><br><span class="line">print(grad)</span><br></pre></td></tr></table></figure>> (tensor([4.]),)          </div><p><strong>backward()</strong></p><p>也可以通过调用 <code>.backward()</code>，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到<code>.grad</code>属性.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line"><span class="comment"># grad = torch.autograd.grad(mse, [w]) 和下面语句等价</span></span><br><span class="line">mse.backward()   <span class="comment"># 不返回值，而是把梯度附加在每个变量的grad属性</span></span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure><blockquote><p>tensor([4.])</p></blockquote><h3 id="调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错："><a href="#调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错：" class="headerlink" title="调用完backward()后，pytorch会把图的信息清除掉，当再次调用backward()，会报错："></a>调用完<code>backward()</code>后，pytorch会把图的信息清除掉，当再次调用<code>backward()</code>，会报错：</h3><blockquote><p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p></blockquote><p>要保持图的信息，可设置<code>retain_graph=True</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(mse, [w], retain_graph=<span class="literal">True</span>)</span><br><span class="line">或</span><br><span class="line">mse.backward(retain_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>detach()</strong></p><p>要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。</p><p>为了防止跟踪历史记录(和使用内存），可以将代码块包装在 <code>with torch.no_grad():</code> 中。在评估模型时特别有用，因为模型可能具有 <code>requires_grad = True</code> 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://pytorch.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch官网&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://pytorch.org/tutorials/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch官方教程&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch官方文档&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://pytorch.apachecn.org/docs/1.4/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch中文文档/教程&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/#/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;动手学深度学习PyTorch版&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://pengwei785.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
