<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PengWei</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://pengwei785.github.io/note/"/>
  <updated>2020-04-21T12:45:27.046Z</updated>
  <id>http://pengwei785.github.io/note/</id>
  
  <author>
    <name>Peng Wei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>配置博客</title>
    <link href="http://pengwei785.github.io/note/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/"/>
    <id>http://pengwei785.github.io/note/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-04-21T12:40:56.000Z</published>
    <updated>2020-04-21T12:45:27.046Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<code>/themes/next/source/css/_common/scaffolding/base.styl  &gt;  a</code></p><p>字体：<code>/themes/next/source/css/_variables/base.styl  &gt;  font-size</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;链接：&lt;code&gt;/themes/next/source/css/_common/scaffolding/base.styl  &amp;gt;  a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;字体：&lt;code&gt;/themes/next/source/css/_variables/base.st
      
    
    </summary>
    
    
      <category term="其它" scheme="http://pengwei785.github.io/note/categories/%E5%85%B6%E5%AE%83/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch</title>
    <link href="http://pengwei785.github.io/note/2020/04/21/PyTorch/"/>
    <id>http://pengwei785.github.io/note/2020/04/21/PyTorch/</id>
    <published>2020-04-21T06:53:16.000Z</published>
    <updated>2020-04-21T12:37:59.865Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch官网</a><br><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch官方教程</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch官方文档</a><br><a href="https://pytorch.apachecn.org/docs/1.4/" target="_blank" rel="noopener">PyTorch中文文档/教程</a><br><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">动手学深度学习PyTorch版</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。</p><p>寻找下面函数的最小值：</p><p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419152635168-692192025.png" alt=""></p><p>conda：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">himmelblau</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>] - <span class="number">11</span>)**<span class="number">2</span> + (x[<span class="number">0</span>] + x[<span class="number">1</span>]**<span class="number">2</span> - <span class="number">7</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> plotly.graph_objects <span class="keyword">as</span> go</span><br><span class="line">x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">y = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># print('x,y range:', x.shape, y.shape)</span></span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">fig = go.Figure(data=go.Surface(z=himmelblau([X,Y])))</span><br><span class="line">fig.write_image(<span class="string">'figure2.svg'</span>)</span><br><span class="line">fig.write_html(<span class="string">'first_figure.html'</span>, auto_open=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = torch.tensor([<span class="number">0.</span>, <span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer = torch.optim.Adam([x], lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    pred = himmelblau(x)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度信息清零</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step() <span class="comment"># 每调用一次step，就更新一次: x' = x, y' = y</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.detach().numpy(), pred.item()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = tf.Variable([<span class="number">0.</span>, <span class="number">0.</span>])  <span class="comment"># 传入GradientTape计算梯度的必须是tf.Variable类型</span></span><br><span class="line">optimizer = tf.optimizers.Adam(lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch([x])</span><br><span class="line">        pred = himmelblau(x)</span><br><span class="line">        </span><br><span class="line">    grads = tape.gradient(pred, [x])</span><br><span class="line">    optimizer.apply_gradients(zip(grads, [x])) <span class="comment"># 和pytorch不同，tf是将所有梯度信息存起来一次性更新</span></span><br><span class="line">    <span class="comment"># x -= 0.001*grads</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.numpy(), pred.numpy()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br></pre></td></tr></table></figure><p>conda版：</p><blockquote><p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.470422983169556</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 43.112674951553345</strong></p></blockquote><p>pip版：</p><blockquote><p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.337981462478638</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 54.814427614212036</strong></p></blockquote><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>新建环境：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name torch python=<span class="number">3.7</span></span><br></pre></td></tr></table></figure><p>安装一些可能要用到的包（非必须，看自己情况）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install numpy</span><br><span class="line">conda install spyder</span><br><span class="line">conda install jupyter notebook</span><br></pre></td></tr></table></figure><p>安装PyTorch：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cpuonly -c pytorch <span class="comment"># CPU版</span></span><br></pre></td></tr></table></figure><p>GPU版根据CUDA版本不同命令也不同，可以去<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">这里</a>查看安装命令</p><h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><p><strong>requires_grad</strong></p><p>设置张量的属性<code>.requires_grad</code> 为 <code>True</code>，那么它将会追踪对于该张量的所有操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>],<span class="number">2</span>)  <span class="comment"># 应为w = torch.full([1], 2, requires_grad=True)</span></span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure><blockquote><p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p></blockquote><h3 id="PyTorch的图是静态创建的，如果使用requires-grad-方法："><a href="#PyTorch的图是静态创建的，如果使用requires-grad-方法：" class="headerlink" title="PyTorch的图是静态创建的，如果使用requires_grad_()方法："></a>PyTorch的图是静态创建的，如果使用<code>requires_grad_()</code>方法：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure><p>依然会报错：</p><blockquote><p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p></blockquote><p>因为mse这张图已经创建好了，因此要重新创建一次，或者将mse的创建放到<code>w.requires_grad_(True)</code>后面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">grad = torch.autograd.grad(mse, [w])  <span class="comment"># 返回一个列表，分别是对每个变量的梯度</span></span><br><span class="line">print(grad)</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([4.]),)</p></blockquote><p><strong>backward()</strong></p><p>也可以通过调用 <code>.backward()</code>，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到<code>.grad</code>属性.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line"><span class="comment"># grad = torch.autograd.grad(mse, [w]) 和下面语句等价</span></span><br><span class="line">mse.backward()   <span class="comment"># 不返回值，而是把梯度附加在每个变量的grad属性</span></span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure><blockquote><p>tensor([4.])</p></blockquote><h3 id="调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错："><a href="#调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错：" class="headerlink" title="调用完backward()后，pytorch会把图的信息清除掉，当再次调用backward()，会报错："></a>调用完<code>backward()</code>后，pytorch会把图的信息清除掉，当再次调用<code>backward()</code>，会报错：</h3><blockquote><p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p></blockquote><p>要保持图的信息，可设置<code>retain_graph=True</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(mse, [w], retain_graph=<span class="literal">True</span>)</span><br><span class="line">或</span><br><span class="line">mse.backward(retain_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>detach()</strong></p><p>要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。</p><p>为了防止跟踪历史记录(和使用内存），可以将代码块包装在 <code>with torch.no_grad():</code> 中。在评估模型时特别有用，因为模型可能具有 <code>requires_grad = True</code> 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://pytorch.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch官网&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://pytorch.org/tutorials/&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://pengwei785.github.io/note/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>test_site</title>
    <link href="http://pengwei785.github.io/note/2020/04/21/test-site/"/>
    <id>http://pengwei785.github.io/note/2020/04/21/test-site/</id>
    <published>2020-04-21T03:08:22.000Z</published>
    <updated>2020-04-21T03:08:22.219Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://pengwei785.github.io/note/2020/04/21/hello-world/"/>
    <id>http://pengwei785.github.io/note/2020/04/21/hello-world/</id>
    <published>2020-04-21T03:05:05.574Z</published>
    <updated>2020-04-21T03:05:05.574Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
