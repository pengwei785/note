{"meta":{"title":"PengWei","subtitle":"","description":"","author":"Peng Wei","url":"http://pengwei785.github.io","root":"/note/"},"pages":[{"title":"tags","date":"2020-04-21T09:29:53.000Z","updated":"2020-04-21T11:28:14.540Z","comments":true,"path":"tags/index.html","permalink":"http://pengwei785.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-21T09:37:52.000Z","updated":"2020-04-21T11:27:50.859Z","comments":true,"path":"categories/index.html","permalink":"http://pengwei785.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"ant","slug":"ant","date":"2020-04-21T15:27:30.000Z","updated":"2020-04-21T15:48:49.353Z","comments":true,"path":"2020/04/21/ant/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/ant/","excerpt":"","text":"","categories":[{"name":"no","slug":"no","permalink":"http://pengwei785.github.io/categories/no/"}],"tags":[]},{"title":"pic","slug":"pic","date":"2020-04-21T15:01:39.000Z","updated":"2020-04-21T15:42:38.483Z","comments":true,"path":"2020/04/21/pic/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/pic/","excerpt":"","text":"","categories":[{"name":"no","slug":"no","permalink":"http://pengwei785.github.io/categories/no/"}],"tags":[]},{"title":"配置博客","slug":"配置博客","date":"2020-04-21T12:40:56.000Z","updated":"2020-04-21T14:36:09.373Z","comments":true,"path":"2020/04/21/配置博客/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"链接：/themes/next/source/css/_common/scaffolding/base.styl &gt; a 字体：/themes/next/source/css/_variables/base.styl &gt; font-size","categories":[{"name":"其它","slug":"其它","permalink":"http://pengwei785.github.io/categories/%E5%85%B6%E5%AE%83/"}],"tags":[]},{"title":"PyTorch","slug":"PyTorch","date":"2020-04-21T06:53:16.000Z","updated":"2020-04-21T14:21:03.739Z","comments":true,"path":"2020/04/21/PyTorch/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/PyTorch/","excerpt":"PyTorch官网PyTorch官方教程PyTorch官方文档PyTorch中文文档/教程动手学深度学习PyTorch版 引言做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。","text":"PyTorch官网PyTorch官方教程PyTorch官方文档PyTorch中文文档/教程动手学深度学习PyTorch版 引言做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。 寻找下面函数的最小值： conda： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import torchimport tensorflow as tfimport timeimport numpy as npdef himmelblau(x): return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2import plotly.graph_objects as gox = np.arange(-6, 6, 0.1)y = np.arange(-6, 6, 0.1)# print('x,y range:', x.shape, y.shape)X, Y = np.meshgrid(x, y)fig = go.Figure(data=go.Surface(z=himmelblau([X,Y])))fig.write_image('figure2.svg')fig.write_html('first_figure.html', auto_open=True)tic = time.time()x = torch.tensor([0., 0.], requires_grad=True)optimizer = torch.optim.Adam([x], lr=1e-3)for step in range(20000): pred = himmelblau(x) optimizer.zero_grad() # 梯度信息清零 pred.backward() optimizer.step() # 每调用一次step，就更新一次: x' = x, y' = y if step % 2000 == 0: print('step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'.format(step, x.detach().numpy(), pred.item()))toc = time.time()print('time:',toc-tic)tic = time.time()x = tf.Variable([0., 0.]) # 传入GradientTape计算梯度的必须是tf.Variable类型optimizer = tf.optimizers.Adam(lr=1e-3)for step in range(20000): with tf.GradientTape() as tape: tape.watch([x]) pred = himmelblau(x) grads = tape.gradient(pred, [x]) optimizer.apply_gradients(zip(grads, [x])) # 和pytorch不同，tf是将所有梯度信息存起来一次性更新 # x -= 0.001*grads if step % 2000 == 0: print('step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'.format(step, x.numpy(), pred.numpy()))toc = time.time()print('time:',toc-tic) conda版： step0: x = [0.001 0.001], f(x) = 170.0step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395step4000: x = [2.982008 2.0270984], f(x) = 0.014858869835734367step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12step16000: x = [3. 2.], f(x) = 0.0step18000: x = [3. 2.], f(x) = 0.0time: 8.470422983169556step0: x = [0.001 0.001], f(x) = 170.0step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12step14000: x = [3. 2.0000002], f(x) = 9.094947017729282e-13step16000: x = [3. 2.], f(x) = 0.0step18000: x = [3. 2.], f(x) = 0.0time: 43.112674951553345 pip版： step0: x = [0.001 0.001], f(x) = 170.0step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395step4000: x = [2.982008 2.0270984], f(x) = 0.014858869835734367step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12step16000: x = [3. 2.], f(x) = 0.0step18000: x = [3. 2.], f(x) = 0.0time: 8.337981462478638step0: x = [0.001 0.001], f(x) = 170.0step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12step14000: x = [3. 2.0000002], f(x) = 9.094947017729282e-13step16000: x = [3. 2.], f(x) = 0.0step18000: x = [3. 2.], f(x) = 0.0time: 54.814427614212036 安装新建环境： 1conda create --name torch python=3.7 安装一些可能要用到的包（非必须，看自己情况）： 123conda install numpyconda install spyderconda install jupyter notebook 安装PyTorch： 1conda install pytorch torchvision cpuonly -c pytorch # CPU版 GPU版根据CUDA版本不同命令也不同，可以去这里查看安装命令 自动求导requires_grad 设置张量的属性.requires_grad 为 True，那么它将会追踪对于该张量的所有操作。 1234567import torchimport torch.nn.functional as Fx = torch.ones(1)w = torch.full([1],2) # 应为w = torch.full([1], 2, requires_grad=True)mse = F.mse_loss(x, x+w)torch.autograd.grad(mse, [w]) RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn PyTorch的图是静态创建的，如果使用requires_grad_()方法：12345x = torch.ones(1)w = torch.full([1], 2)mse = F.mse_loss(x, x+w)w.requires_grad_()torch.autograd.grad(mse, [w]) 依然会报错： RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn 因为mse这张图已经创建好了，因此要重新创建一次，或者将mse的创建放到w.requires_grad_(True)后面： 123456x = torch.ones(1)w = torch.full([1], 2)w.requires_grad_()mse = F.mse_loss(x, x+w)grad = torch.autograd.grad(mse, [w]) # 返回一个列表，分别是对每个变量的梯度print(grad) (tensor([4.]),) backward() 也可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性. 1234567x = torch.ones(1)w = torch.full([1], 2)w.requires_grad_()mse = F.mse_loss(x, x+w)# grad = torch.autograd.grad(mse, [w]) 和下面语句等价mse.backward() # 不返回值，而是把梯度附加在每个变量的grad属性print(w.grad) tensor([4.]) 调用完backward()后，pytorch会把图的信息清除掉，当再次调用backward()，会报错： RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time. 要保持图的信息，可设置retain_graph=True 123torch.autograd.grad(mse, [w], retain_graph=True)或mse.backward(retain_graph=True) detach() 要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。 为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://pengwei785.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"test_site","slug":"test-site","date":"2020-04-21T03:08:22.000Z","updated":"2020-04-21T14:38:05.130Z","comments":true,"path":"2020/04/21/test-site/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/test-site/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-04-21T03:05:05.574Z","updated":"2020-04-21T14:21:18.594Z","comments":true,"path":"2020/04/21/hello-world/","link":"","permalink":"http://pengwei785.github.io/2020/04/21/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"no","slug":"no","permalink":"http://pengwei785.github.io/categories/no/"},{"name":"其它","slug":"其它","permalink":"http://pengwei785.github.io/categories/%E5%85%B6%E5%AE%83/"},{"name":"深度学习","slug":"深度学习","permalink":"http://pengwei785.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]}