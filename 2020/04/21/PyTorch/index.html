<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/note/img/favicon.ico">

    <title>
        
        PyTorch - undefined
        
    </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/note/css/aircloud.css">

    
<link rel="stylesheet" href="/note/css/gitment.css">

    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/note/atom.xml" title="PengWei" type="application/atom+xml">
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/note/" />
        </div>
        <div class="name">
            <i>Peng Wei</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/note/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/note/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/note/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/note/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#引言"><span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#安装"><span class="toc-text">安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自动求导"><span class="toc-text">自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch的图是静态创建的，如果使用requires-grad-方法："><span class="toc-text">PyTorch的图是静态创建的，如果使用requires_grad_()方法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错："><span class="toc-text">调用完backward()后，pytorch会把图的信息清除掉，当再次调用backward()，会报错：</span></a></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        PyTorch
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2020-04-21 14:53:16</span></span>
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <p><a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch官网</a><br><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch官方教程</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch官方文档</a><br><a href="https://pytorch.apachecn.org/docs/1.4/" target="_blank" rel="noopener">PyTorch中文文档/教程</a><br><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">动手学深度学习PyTorch版</a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>做了一个小测试，发现在cpu上pytorch比tensorflow快很多。另外还发现，conda命令安装的tensorflow比pip安装的要快，pytorch则没有明显区别，之前就看到有人说conda中的tensorflow经过了优化，看来是真的。</p>
<p>寻找下面函数的最小值：</p>
<p><img src="https://img2020.cnblogs.com/blog/1758578/202004/1758578-20200419152635168-692192025.png" alt=""></p>
<p>conda：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">himmelblau</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>] - <span class="number">11</span>)**<span class="number">2</span> + (x[<span class="number">0</span>] + x[<span class="number">1</span>]**<span class="number">2</span> - <span class="number">7</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> plotly.graph_objects <span class="keyword">as</span> go</span><br><span class="line">x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">y = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># print('x,y range:', x.shape, y.shape)</span></span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">fig = go.Figure(data=go.Surface(z=himmelblau([X,Y])))</span><br><span class="line">fig.write_image(<span class="string">'figure2.svg'</span>)</span><br><span class="line">fig.write_html(<span class="string">'first_figure.html'</span>, auto_open=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = torch.tensor([<span class="number">0.</span>, <span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer = torch.optim.Adam([x], lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    pred = himmelblau(x)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度信息清零</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step() <span class="comment"># 每调用一次step，就更新一次: x' = x, y' = y</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.detach().numpy(), pred.item()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">x = tf.Variable([<span class="number">0.</span>, <span class="number">0.</span>])  <span class="comment"># 传入GradientTape计算梯度的必须是tf.Variable类型</span></span><br><span class="line">optimizer = tf.optimizers.Adam(lr=<span class="number">1e-3</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch([x])</span><br><span class="line">        pred = himmelblau(x)</span><br><span class="line">        </span><br><span class="line">    grads = tape.gradient(pred, [x])</span><br><span class="line">    optimizer.apply_gradients(zip(grads, [x])) <span class="comment"># 和pytorch不同，tf是将所有梯度信息存起来一次性更新</span></span><br><span class="line">    <span class="comment"># x -= 0.001*grads</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step&#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.numpy(), pred.numpy()))</span><br><span class="line">toc = time.time()</span><br><span class="line">print(<span class="string">'time:'</span>,toc-tic)</span><br></pre></td></tr></table></figure>
<p>conda版：</p>
<blockquote>
<p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.470422983169556</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 43.112674951553345</strong></p>
</blockquote>
<p>pip版：</p>
<blockquote>
<p>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331807 1.9540695], f(x) = 13.730916023254395<br>step4000: x = [2.982008  2.0270984], f(x) = 0.014858869835734367<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999993 2.000001 ], f(x) = 1.6370904631912708e-11<br>step14000: x = [2.9999998 2.0000002], f(x) = 1.8189894035458565e-12<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 8.337981462478638</strong><br>step0: x = [0.001 0.001], f(x) = 170.0<br>step2000: x = [2.3331852 1.9540718], f(x) = 13.730728149414062<br>step4000: x = [2.9820085 2.0270977], f(x) = 0.01485812570899725<br>step6000: x = [2.9999835 2.0000222], f(x) = 1.1074007488787174e-08<br>step8000: x = [2.9999938 2.0000083], f(x) = 1.5572823031106964e-09<br>step10000: x = [2.9999979 2.0000029], f(x) = 1.8189894035458565e-10<br>step12000: x = [2.9999995 2.0000007], f(x) = 9.322320693172514e-12<br>step14000: x = [3.        2.0000002], f(x) = 9.094947017729282e-13<br>step16000: x = [3. 2.], f(x) = 0.0<br>step18000: x = [3. 2.], f(x) = 0.0<br><strong>time: 54.814427614212036</strong></p>
</blockquote>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>新建环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name torch python=<span class="number">3.7</span></span><br></pre></td></tr></table></figure>

<p>安装一些可能要用到的包（非必须，看自己情况）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install numpy</span><br><span class="line">conda install spyder</span><br><span class="line">conda install jupyter notebook</span><br></pre></td></tr></table></figure>

<p>安装PyTorch：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cpuonly -c pytorch <span class="comment"># CPU版</span></span><br></pre></td></tr></table></figure>
<p>GPU版根据CUDA版本不同命令也不同，可以去<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">这里</a>查看安装命令</p>
<h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><p><strong>requires_grad</strong></p>
<p>设置张量的属性<code>.requires_grad</code> 为 <code>True</code>，那么它将会追踪对于该张量的所有操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>],<span class="number">2</span>)  <span class="comment"># 应为w = torch.full([1], 2, requires_grad=True)</span></span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>
<h3 id="PyTorch的图是静态创建的，如果使用requires-grad-方法："><a href="#PyTorch的图是静态创建的，如果使用requires-grad-方法：" class="headerlink" title="PyTorch的图是静态创建的，如果使用requires_grad_()方法："></a>PyTorch的图是静态创建的，如果使用<code>requires_grad_()</code>方法：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">torch.autograd.grad(mse, [w])</span><br></pre></td></tr></table></figure>
<p>依然会报错：</p>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>
<p>因为mse这张图已经创建好了，因此要重新创建一次，或者将mse的创建放到<code>w.requires_grad_(True)</code>后面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line">grad = torch.autograd.grad(mse, [w])  <span class="comment"># 返回一个列表，分别是对每个变量的梯度</span></span><br><span class="line">print(grad)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([4.]),)</p>
</blockquote>
<p><strong>backward()</strong></p>
<p>也可以通过调用 <code>.backward()</code>，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到<code>.grad</code>属性.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.full([<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse = F.mse_loss(x, x+w)</span><br><span class="line"><span class="comment"># grad = torch.autograd.grad(mse, [w]) 和下面语句等价</span></span><br><span class="line">mse.backward()   <span class="comment"># 不返回值，而是把梯度附加在每个变量的grad属性</span></span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([4.])</p>
</blockquote>
<h3 id="调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错："><a href="#调用完backward-后，pytorch会把图的信息清除掉，当再次调用backward-，会报错：" class="headerlink" title="调用完backward()后，pytorch会把图的信息清除掉，当再次调用backward()，会报错："></a>调用完<code>backward()</code>后，pytorch会把图的信息清除掉，当再次调用<code>backward()</code>，会报错：</h3><blockquote>
<p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p>
</blockquote>
<p>要保持图的信息，可设置<code>retain_graph=True</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(mse, [w], retain_graph=<span class="literal">True</span>)</span><br><span class="line">或</span><br><span class="line">mse.backward(retain_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>detach()</strong></p>
<p>要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。</p>
<p>为了防止跟踪历史记录(和使用内存），可以将代码块包装在 <code>with torch.no_grad():</code> 中。在评估模型时特别有用，因为模型可能具有 <code>requires_grad = True</code> 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud" target="_blank" rel="noopener">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/note/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

<script src="/note/js/index.js"></script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
